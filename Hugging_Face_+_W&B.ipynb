{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkhfring/parallel-c/blob/main/Hugging_Face_%2B_W%26B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccHJqOajNN7l"
      },
      "source": [
        "#Hugging Face + Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKpiNY516JUf"
      },
      "source": [
        "### Introduction\n",
        "Visualize your [Hugging Face](https://github.com/huggingface/transformers) model's performance quickly with a seamless W&B integration. Compare hyperparameters, output metrics, and system stats like GPU utilization across your models. \n",
        "\n",
        "<img src=\"https://i.imgur.com/vnejHGh.png\" width=\"800\">\n",
        "\n",
        "**Why use W&B**\n",
        "\n",
        "Think of W&B like GitHub for machine learning models— save machine learning experiments to your private, hosted dashboard. Experiment quickly with the confidence that all the versions of your models are saved for you, no matter where you're running your scripts.\n",
        "\n",
        "W&B lightweight integrations works with any Python script, and all you need to do is sign up for a free W&B account to start tracking and visualizing your models.\n",
        "\n",
        "In the Hugging Face Transformers repo, we've instrumented the Trainer to automatically log training and evaluation metrics to W&B at each logging step.\n",
        "\n",
        "Here's an in depth look at how the integration works: [Hugging Face + W&B Report](https://app.wandb.ai/jxmorris12/huggingface-demo/reports/Train-a-model-with-Hugging-Face-and-Weights-%26-Biases--VmlldzoxMDE2MTU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt71k9Ew6Qp6"
      },
      "source": [
        "### Install dependencies\n",
        "Install the Hugging Face and Weights & Biases libraries, and the GLUE dataset and training script for this tutorial.\n",
        "- [Hugging Face Transformers](https://github.com/huggingface/transformers): Natural language models and datasets\n",
        "- [Weights & Biases](https://docs.wandb.com/): Experiment tracking and visualization\n",
        "- [GLUE dataset](https://gluebenchmark.com/): A language understanding benchmark dataset\n",
        "- [GLUE script](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py): Model training script for sequence classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUP7zxN2NKUi"
      },
      "source": [
        "!pip install transformers datasets -qq\n",
        "!pip install wandb -qq\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/text-classification/run_glue.py -qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directories = os.listdir()\n",
        "\n",
        "program_dirs = [element for element in directories if 'program' in element]\n",
        "print(program_dirs)\n",
        "print(os.listdir(program_dirs[0]))"
      ],
      "metadata": {
        "id": "5_UUmEJT0YrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dGrQ-cdzK6cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "fCTRS6R8LnsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directories = os.listdir()\n",
        "print(directories)\n",
        "program_dirs = [element for element in directories if 'p' in element]\n",
        "\n",
        "print(program_dirs)"
      ],
      "metadata": {
        "id": "6EEV3S8lL5aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjX-QzE257Zv"
      },
      "source": [
        "### [Sign up for a free account →](https://app.wandb.ai/login?signup=true)\n",
        "\n",
        "- **Unified dashboard**: Central repository for all your model metrics and predictions\n",
        "- **Lightweight**: No code changes required to integrate with Hugging Face\n",
        "- **Accessible**: Free for individuals and academic teams\n",
        "- **Secure**: All projects are private by default\n",
        "- **Trusted**: Used by machine learning teams at OpenAI, Toyota, GitHub, Lyft and more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5DsG_bALsh"
      },
      "source": [
        "## API Key\n",
        "Once you've signed up, run the next cell and click on the link to get your API key and authenticate this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "id": "vUCOTZu5vf3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "B_YUpkeyY98A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the CodeBERT model\n",
        "model = transformers.AutoModel.from_pretrained('microsoft/codebert-base')\n",
        "\n",
        "# Define the two code snippets that you want to compare\n",
        "def similarity(em1, em2):\n",
        "  em1 = em1.detach().numpy()\n",
        "  em2 = em2.detach().numpy()\n",
        "  dot_product = np.dot(em1, em2)\n",
        "  norm1 = np.linalg.norm(em1)\n",
        "  norm2 = np.linalg.norm(em2)\n",
        "\n",
        "  similarity = dot_product / (norm1 * norm2)\n",
        "  \n",
        "  return similarity\n",
        "\n",
        "code1 = \"\"\"\n",
        "import java.util.ArrayList;\n",
        "import java.util.Collections;\n",
        "import java.util.List;\n",
        "import java.util.Scanner;\n",
        "\n",
        "public class Main {\n",
        "    private static Scanner scan;\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        scan = new Scanner(System.in);\n",
        "        List<Integer> list = new ArrayList<Integer>();\n",
        "        for (int i = 0; i < 10; i++) {\n",
        "            list.add(scan.nextInt());\n",
        "        }\n",
        "        Collections.sort(list);\n",
        "        System.out.println(list.get(9) + \"\\n\");\n",
        "        System.out.println(list.get(8) + \"\\n\");\n",
        "        System.out.println(list.get(7) + \"\\n\");\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Tokenize the codes\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
        "input_ids1 = tokenizer.encode(code1, return_tensors='pt')\n",
        "\n",
        "\n",
        "for p in program_dirs:\n",
        "  p_similarity = []\n",
        "  files = [f for f in os.listdir(p) if 'java' in f]\n",
        "  for file in files:\n",
        "    sim = []\n",
        "    f = open(os.path.join(p, file))\n",
        "    code2 = f.read()\n",
        "    input_ids2 = tokenizer.encode(code2, return_tensors='pt')\n",
        "# Compute the embeddings\n",
        "    embedding2 = model(input_ids1).last_hidden_state\n",
        "    embedding1 = model(input_ids2).last_hidden_state\n",
        "# print(embedding2.shape)\n",
        "# print(embedding1.shape)\n",
        "    if embedding1.shape[1] > embedding2.shape[1]:\n",
        "      em = embedding1\n",
        "      embedding1  = embedding2\n",
        "      embedding2 = em\n",
        "\n",
        "# print(\"After relplace\")\n",
        "# print(embedding2.shape)\n",
        "# print(embedding1.shape)\n",
        "    sum = 0\n",
        "    for i in range(embedding1.shape[1]-1):\n",
        "      sim.append(similarity(embedding1[0][i], embedding2[0][i]))\n",
        "      sum = sum + similarity(embedding1[0][i], embedding2[0][i])\n",
        "\n",
        "\n",
        "  #   p_similarity.append( float(sum/ int(embedding1.shape[1])))\n",
        "  # print(p_similarity)\n",
        "  print(sim)\n",
        "  print(f\"the similarity for problem {p} is {sum / len(sim)}\")\n",
        "  # print(\"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LP6I7PvaOLRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "RDA8-umfI48w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.language_representation import RepresentationModel\n",
        "code1 = \"\"\"\n",
        "import java.util.ArrayList;\n",
        "import java.util.Collections;\n",
        "import java.util.List;\n",
        "import java.util.Scanner;\n",
        "\n",
        "public class Main {\n",
        "    private static Scanner scan;\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        scan = new Scanner(System.in);\n",
        "        List<Integer> list = new ArrayList<Integer>();\n",
        "        for (int i = 0; i < 10; i++) {\n",
        "            list.add(scan.nextInt());\n",
        "        }\n",
        "        Collections.sort(list);\n",
        "        System.out.println(list.get(9) + \"\\n\");\n",
        "        System.out.println(list.get(8) + \"\\n\");\n",
        "        System.out.println(list.get(7) + \"\\n\");\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "sentences = [ code1] \n",
        "print(program_dirs)\n",
        "for p in program_dirs:\n",
        "  p_similarity = []\n",
        "  files = [f for f in os.listdir(p) if 'java' in f]\n",
        "  for file in files:\n",
        "    f = open(os.path.join(p, file))\n",
        "    sentences.append(f.read())\n",
        "\n",
        "print(sentences)\n",
        "        \n",
        "sentences = [\"Machine Learning and Deep Learning are part of AI\", code1] #it should always be a list\n",
        "\n",
        "model = RepresentationModel(\n",
        "        model_type=\"bert\",\n",
        "        model_name=\"bert-base-uncased\",\n",
        "        use_cuda=True\n",
        "    )\n",
        "\n",
        "word_vectors = model.encode_sentences(sentences, combine_strategy=None)\n",
        "     "
      ],
      "metadata": {
        "id": "WXKnm86OI89x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.shape"
      ],
      "metadata": {
        "id": "6M3RodBfJKYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9nzcJmLO0T_V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ45eea3Cujc"
      },
      "source": [
        "## Resources\n",
        "- [Documentation](https://docs.wandb.com/huggingface): docs on the Weights & Biases and Hugging Face integration\n",
        "- Contact: Message us at contact@wandb.com with questions"
      ]
    }
  ]
}