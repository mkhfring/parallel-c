{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "54hsYo44eQ1Y"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkhfring/parallel-c/blob/main/TSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "1.   **GPU Runtime**: click on the \"Runtime\" menu item in the top bar and select the \"Change runtime type\" option. Select \"GPU\" from the list of Hardware accelerators and click \"Ok\".  \n",
        "\n",
        "2.   CUDA Compilation: we will use of the NVCC4Jupyter plugin which effectively turns any Colab Notebook code block that includes `%%cu` into compilable/runnable CUDA code."
      ],
      "metadata": {
        "id": "O7RFj9ykaSzY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYXTE96HMb_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b22ea5-5ff2-44e5-813f-1fb49e34f75e"
      },
      "source": [
        "# first run this to install and load nvcc plugin \n",
        "!pip install git+https://github.com/engasa/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/engasa/nvcc4jupyter.git\n",
            "  Cloning https://github.com/engasa/nvcc4jupyter.git to /tmp/pip-req-build-f088wn00\n",
            "  Running command git clone -q https://github.com/engasa/nvcc4jupyter.git /tmp/pip-req-build-f088wn00\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4406 sha256=fe4cbecd5092fb37111aee8d68f2476cc5b0631a6b3069f848192ecd03883355\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qlpvjttd/wheels/36/86/36/c7b00095a61c28f9bf69a386c706b14b45c600ce89dc6c16b2\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3.   Now you can check your CUDA installation by running the command below. The output should show you some info about the Cuda compiler, e.g., \"*nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021* ...etc\""
      ],
      "metadata": {
        "id": "7MwvWlydb9W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check nvcc version\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "I1ZHiWr-cNJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c867885-3983-44d3-f453-cc2d3908a63b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   You can also check if GPU has been allocated. Colab notebooks without a GPU technically have access to NVCC and will compile and execute CPU/Host code, however, GPU/Device code will silently fail. To prevent such situations, this code will warn the user.\n"
      ],
      "metadata": {
        "id": "x-dAfZOCiayz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "int main() {\n",
        "    int count;\n",
        "    cudaGetDeviceCount(&count);\n",
        "    if (count <= 0 || count > 100)  printf(\"!!!!! WARNING<-: NO GPU DETECTED ON THIS COLLABORATE INSTANCE. YOU SHOULD CHANGE THE RUNTIME TYPE.!!!!!\\n\");\n",
        "    else                            printf(\"^^^^ GPU ENABLED! ^^^^\\n\");\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "W1IWUSIFiSkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2cca7f-4e6c-4d06-b2f7-bb1e56e5d65d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^^^^ GPU ENABLED! ^^^^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "__host__\n",
        "void initialize(int8_t * city_ids, int8_t * graphWeights, int32_t size) {\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tcity_ids[i] = i;\n",
        "\t\tfor (int j = 0; j < size; j++) {\n",
        "\t\t\tif (i == j)\n",
        "\t\t\t\tgraphWeights[i * size + j] = 0;\n",
        "\t\t\telse\n",
        "\t\t\t\tgraphWeights[i * size + j] = 99;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tfor (int j = 0; j < size;) {\n",
        "\t\t\tint next = 1; // (rand() % 2) + 1;\n",
        "\t\t\tint road = rand() % 100 + 1;\n",
        "\t\t\tif (i == j) {\n",
        "\t\t\t\tj += next;\n",
        "\t\t\t\tcontinue;\n",
        "\t\t\t}\n",
        "\t\t\tgraphWeights[i * size + j] = road;\n",
        "\t\t\tj += next;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = size - 1; i >= 0; i--) {\n",
        "\t\tgraphWeights[((i + 1) % size) * size + i] = 1;\n",
        "\t}\n",
        "}"
      ],
      "metadata": {
        "id": "qJwujAh3YK99",
        "outputId": "ea79b2ea-6b95-4fdf-e469-064964d4b118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/Scrt1.o: In function `_start':\n",
            "(.text+0x20): undefined reference to `main'\n",
            "collect2: error: ld returned 1 exit status\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <stdint.h>\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#define MAX_THREADS 1024\n",
        "#define MAX_BLOCKS 30\n",
        "#define MAX_PERMS 5041\n",
        "\n",
        "#define CUDA_RUN(x_) {cudaError_t cudaStatus = x_; if (cudaStatus != cudaSuccess) {fprintf(stderr, \"Error  %d - %s\\n\", cudaStatus, cudaGetErrorString(cudaStatus)); goto Error;}}\n",
        "#define SAFE(x_) {if((x_) == NULL) printf(\"out of memory. %d\\n\", __LINE__);}\n",
        "\n",
        "__host__ unsigned long long factorial(int32_t n);\n",
        "//_host__ void initialize(int8_t * city_ids, int8_t * graphWeights, int32_t size);\n",
        "\n",
        "__host__ unsigned long long factorial(int32_t n) {\n",
        "\tint c;\n",
        "\tunsigned long long result = 1;\n",
        "\n",
        "\tfor (c = 1; c <= n; c++){\n",
        "\t\tresult = result * c;\n",
        "  }\n",
        "\n",
        "\treturn result;\n",
        "}\n",
        "\n",
        "__host__\n",
        "void initialize(int8_t * city_ids, int8_t * graphWeights, int32_t size) {\n",
        "  printf(\"Initializing the problem\\n\");\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tcity_ids[i] = i;\n",
        "\t\tfor (int j = 0; j < size; j++) {\n",
        "\t\t\tif (i == j)\n",
        "\t\t\t\tgraphWeights[i * size + j] = 0;\n",
        "\t\t\telse\n",
        "\t\t\t\tgraphWeights[i * size + j] = 99;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tfor (int j = 0; j < size;) {\n",
        "\t\t\tint next = 1; // (rand() % 2) + 1;\n",
        "\t\t\tint road = rand() % 100 + 1;\n",
        "\t\t\tif (i == j) {\n",
        "\t\t\t\tj += next;\n",
        "\t\t\t\tcontinue;\n",
        "\t\t\t}\n",
        "\t\t\tgraphWeights[i * size + j] = road;\n",
        "\t\t\tj += next;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = size - 1; i >= 0; i--) {\n",
        "\t\tgraphWeights[((i + 1) % size) * size + i] = 1;\n",
        "\t}\n",
        "  for(int i=0; i<size; i++){\n",
        "      for(int j=0; j<size; j++){\n",
        "          printf(\"%d,\\t\", graphWeights[i * size + j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\tint size8 = sizeof(int8_t);\n",
        "\tint size32 = sizeof(int32_t);\n",
        "\tunsigned long long total_permutations, thread_perms, num_blocks = 1, num_threads, num_kernels = 1;\n",
        "\tfloat time_passed;\n",
        "\tcudaEvent_t startEvent, stopEvent;\n",
        "\t/* host variables */\n",
        "\tint8_t * city_ids, *shortestPath, *graphWeights, *choices;\n",
        "  int32_t size = 5, *cost;\n",
        "\tint8_t selected_K = 0;\n",
        "\tunsigned long long threads_per_kernel;\n",
        "\t/* device variables */\n",
        "\tint8_t * dev_city_ids, *dev_shortestPath, *dev_graphWeights, *dev_choices;\n",
        "\tint32_t * dev_cost, *dev_size;\n",
        "\tint8_t * dev_selected_K;\n",
        "\tunsigned long long * dev_threads_per_kernel;\n",
        "\n",
        "\ttotal_permutations = factorial(size - 1);\n",
        "\tprintf(\"factorial(%d): %llu\\n\", size - 1, total_permutations);\n",
        "\n",
        "\tfor (selected_K = 1; selected_K < size - 2; selected_K++) {\n",
        "\t\tthread_perms = factorial(size - 1 - selected_K);\n",
        "\t\tif (thread_perms < MAX_PERMS) break;\n",
        "\t}\n",
        "  \n",
        "\tnum_threads = total_permutations / thread_perms;\n",
        "\tint k;\n",
        "\twhile (num_threads > MAX_THREADS) {\n",
        "\t\tk = 2;\n",
        "\t\twhile (num_threads % k != 0) k++;\n",
        "\t\tnum_threads /= k;\n",
        "\t\tnum_blocks *= k;\n",
        "\t}\n",
        "\twhile (num_blocks > MAX_BLOCKS) {\n",
        "\t\tk = 2;\n",
        "\t\twhile (num_blocks % k != 0) k++;\n",
        "\t\tnum_blocks /= k;\n",
        "\t\tnum_kernels *= k;\n",
        "\t}\n",
        "\tthreads_per_kernel = num_blocks * num_threads;\n",
        "\tprintf(\"K selected: %d\\n\", selected_K);\n",
        "\tprintf(\"num_threads %llu thread_perms %llu num_blocks %llu num_kernels %llu threads_per_kernel %llu\\n\", num_threads, thread_perms, num_blocks, num_kernels, threads_per_kernel);\n",
        "\n",
        "\tdim3 block_dim(num_threads, 1, 1);\n",
        "\tdim3 grid_dim(num_blocks, 1, 1);\n",
        "  SAFE(city_ids = (int8_t *)malloc(size * size8));\n",
        "\tSAFE(shortestPath = (int8_t *)calloc(num_blocks * size, size8));\n",
        "\tSAFE(graphWeights = (int8_t *)malloc(size * size8 * size));\n",
        "\tSAFE(cost = (int32_t *)calloc(num_blocks * size, size32));\n",
        "\tSAFE(choices = (int8_t *)malloc(threads_per_kernel * size * size8));\n",
        "\n",
        "  CUDA_RUN(cudaMalloc((void **)&dev_city_ids, size * size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_shortestPath, size * size8 * num_blocks));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_graphWeights, size * size8 * size));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_cost, num_blocks * size32));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_size, size32));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_selected_K, size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_choices, threads_per_kernel * size * size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_threads_per_kernel, sizeof(unsigned long long)));\n",
        "\n",
        "  srand(time(NULL));\n",
        "\tinitialize(city_ids, graphWeights, size);\n",
        "\n",
        "\tCUDA_RUN(cudaMemcpy(dev_city_ids, city_ids, size * size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_shortestPath, shortestPath, size * size8 * num_blocks, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_graphWeights, graphWeights, size * size8 * size, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_size, &size, size32, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_selected_K, &selected_K, size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_choices, choices, threads_per_kernel * size * size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_threads_per_kernel, &threads_per_kernel, sizeof(unsigned long long), cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_cost, cost, num_blocks * size32, cudaMemcpyHostToDevice));\n",
        "\n",
        "  Error:\n",
        "\tfree(city_ids);\n",
        "\tfree(shortestPath);\n",
        "\tfree(graphWeights);\n",
        "\tfree(cost);\n",
        "\tfree(choices);\n",
        "\n",
        "\tcudaFree(dev_city_ids);\n",
        "\tcudaFree(dev_shortestPath);\n",
        "\tcudaFree(dev_graphWeights);\n",
        "\tcudaFree(dev_cost);\n",
        "\tcudaFree(dev_size);\n",
        "\tcudaFree(dev_selected_K);\n",
        "\tcudaFree(dev_choices);\n",
        "\tcudaFree(dev_threads_per_kernel);\n",
        "\n",
        "  \n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "0AjgY_rGKuh-",
        "outputId": "d628f16d-a0b9-4987-8636-f2ec0bf41d74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "factorial(4): 24\n",
            "K selected: 1\n",
            "num_threads 4 thread_perms 6 num_blocks 1 num_kernels 1 threads_per_kernel 4\n",
            "Initializing the problem\n",
            "0,\t60,\t95,\t46,\t1,\t\n",
            "1,\t0,\t21,\t79,\t96,\t\n",
            "40,\t1,\t0,\t6,\t60,\t\n",
            "88,\t13,\t1,\t0,\t99,\t\n",
            "85,\t46,\t72,\t1,\t0,\t\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <stdint.h>\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "// __device__ void * cudaMemmove(void * dst0, const void * src0, register size_t length);\n",
        "__device__ void swap(int8_t *x, int8_t *y);\n",
        "__device__ void reverse(int8_t *first, int8_t *last);\n",
        "__device__ void coppy_array(int8_t * _path, int8_t *_shortestPath, int32_t * _tcost, int8_t * weights, int8_t length, int tid);\n",
        "__device__ bool next_permutation(int8_t * first, int8_t * last);\n",
        "__global__ void find_permutations_for_threads(int8_t * city_ids, int8_t * k, int8_t * choices, int32_t * size, unsigned long long * perm_counter);\n",
        "__global__ void combinations_kernel(int8_t * choices, int8_t * k, int8_t * shortestPath, int8_t * graphWeights, int32_t * cost, int32_t * size);\n",
        "__host__ void initialize(int8_t * city_ids, int8_t * graphWeights, int32_t size);\n",
        "__host__ void print_Graph(int8_t * graphWeights, int32_t size);\n",
        "__host__ void print_ShortestPath(int8_t * shortestPath, int32_t cost, int32_t size);\n",
        "__host__ unsigned long long factorial(int32_t n);\n",
        "\n",
        "#define MAX_THREADS 1024\n",
        "#define MAX_BLOCKS 30\n",
        "#define MAX_PERMS 5041\n",
        "\n",
        "#define CUDA_RUN(x_) {cudaError_t cudaStatus = x_; if (cudaStatus != cudaSuccess) {fprintf(stderr, \"Error  %d - %s\\n\", cudaStatus, cudaGetErrorString(cudaStatus)); goto Error;}}\n",
        "#define SAFE(x_) {if((x_) == NULL) printf(\"out of memory. %d\\n\", __LINE__);}\n",
        "\n",
        "__device__ __shared__ int32_t shared_cost;\n",
        "\n",
        "__host__ unsigned long long factorial(int32_t n) {\n",
        "\tint c;\n",
        "\tunsigned long long result = 1;\n",
        "\n",
        "\tfor (c = 1; c <= n; c++)\n",
        "\t\tresult = result * c;\n",
        "\n",
        "\treturn result;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\tint size8 = sizeof(int8_t);\n",
        "\tint size32 = sizeof(int32_t);\n",
        "\tunsigned long long total_permutations, thread_perms, num_blocks = 1, num_threads, num_kernels = 1;\n",
        "\tfloat time_passed;\n",
        "\tcudaEvent_t startEvent, stopEvent;\n",
        "\t/* host variables */\n",
        "\tint8_t * city_ids, *shortestPath, *graphWeights, *choices;\n",
        "\t//int32_t size = atoi(argv[1]), *cost;\n",
        "  int32_t size = 100, *cost;\n",
        "\tint8_t selected_K = 0;\n",
        "\tunsigned long long threads_per_kernel;\n",
        "\t/* device variables */\n",
        "\tint8_t * dev_city_ids, *dev_shortestPath, *dev_graphWeights, *dev_choices;\n",
        "\tint32_t * dev_cost, *dev_size;\n",
        "\tint8_t * dev_selected_K;\n",
        "\tunsigned long long * dev_threads_per_kernel;\n",
        "\n",
        "\ttotal_permutations = factorial(size - 1);\n",
        "\tprintf(\"factorial(%d): %llu\\n\", size - 1, total_permutations);\n",
        "\n",
        "\tfor (selected_K = 1; selected_K < size - 2; selected_K++) {\n",
        "\t\tthread_perms = factorial(size - 1 - selected_K);\n",
        "\t\tif (thread_perms < MAX_PERMS) break;\n",
        "\t}\n",
        "\tnum_threads = total_permutations / thread_perms;\n",
        "\tint k;\n",
        "\twhile (num_threads > MAX_THREADS) {\n",
        "\t\tk = 2;\n",
        "\t\twhile (num_threads % k != 0) k++;\n",
        "\t\tnum_threads /= k;\n",
        "\t\tnum_blocks *= k;\n",
        "\t}\n",
        "\twhile (num_blocks > MAX_BLOCKS) {\n",
        "\t\tk = 2;\n",
        "\t\twhile (num_blocks % k != 0) k++;\n",
        "\t\tnum_blocks /= k;\n",
        "\t\tnum_kernels *= k;\n",
        "\t}\n",
        "\tthreads_per_kernel = num_blocks * num_threads;\n",
        "\tprintf(\"K selected: %d\\n\", selected_K);\n",
        "\tprintf(\"num_threads %llu thread_perms %llu num_blocks %llu num_kernels %llu threads_per_kernel %llu\\n\", num_threads, thread_perms, num_blocks, num_kernels, threads_per_kernel);\n",
        "\n",
        "\tdim3 block_dim(num_threads, 1, 1);\n",
        "\tdim3 grid_dim(num_blocks, 1, 1);\n",
        "\n",
        "\tSAFE(city_ids = (int8_t *)malloc(size * size8));\n",
        "\tSAFE(shortestPath = (int8_t *)calloc(num_blocks * size, size8));\n",
        "\tSAFE(graphWeights = (int8_t *)malloc(size * size8 * size));\n",
        "\tSAFE(cost = (int32_t *)calloc(num_blocks * size, size32));\n",
        "\tSAFE(choices = (int8_t *)malloc(threads_per_kernel * size * size8));\n",
        "\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_city_ids, size * size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_shortestPath, size * size8 * num_blocks));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_graphWeights, size * size8 * size));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_cost, num_blocks * size32));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_size, size32));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_selected_K, size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_choices, threads_per_kernel * size * size8));\n",
        "\tCUDA_RUN(cudaMalloc((void **)&dev_threads_per_kernel, sizeof(unsigned long long)));\n",
        "\n",
        "\tsrand(time(NULL));\n",
        "\tinitialize(city_ids, graphWeights, size);\n",
        "\n",
        "\tCUDA_RUN(cudaMemcpy(dev_city_ids, city_ids, size * size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_shortestPath, shortestPath, size * size8 * num_blocks, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_graphWeights, graphWeights, size * size8 * size, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_size, &size, size32, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_selected_K, &selected_K, size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_choices, choices, threads_per_kernel * size * size8, cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_threads_per_kernel, &threads_per_kernel, sizeof(unsigned long long), cudaMemcpyHostToDevice));\n",
        "\tCUDA_RUN(cudaMemcpy(dev_cost, cost, num_blocks * size32, cudaMemcpyHostToDevice));\n",
        "\n",
        "\tCUDA_RUN(cudaEventCreate(&startEvent));\n",
        "\tCUDA_RUN(cudaEventCreate(&stopEvent));\n",
        "\tCUDA_RUN(cudaEventRecord(startEvent, 0));\n",
        "\tfloat percentage;\n",
        "\tfor (int i = 0; i < num_kernels; i++) {\n",
        "\t\tfind_permutations_for_threads << < 1, 1 >> >(dev_city_ids, dev_selected_K, dev_choices, dev_size, dev_threads_per_kernel);\n",
        "\t\tCUDA_RUN(cudaGetLastError());\n",
        "\t\tCUDA_RUN(cudaDeviceSynchronize());\n",
        "\t\tcombinations_kernel << < grid_dim, block_dim >> > (dev_choices, dev_selected_K, dev_shortestPath, dev_graphWeights, dev_cost, dev_size);\n",
        "\t\tCUDA_RUN(cudaGetLastError());\n",
        "\t\tCUDA_RUN(cudaDeviceSynchronize());\n",
        "\t\tpercentage = (100. / (float) num_kernels * (float)(i + 1));\n",
        "\t\tprintf(\"\\rProgress : \");\n",
        "\t\tfor (int j = 0; j < 10; j++) {\n",
        "\t\t\tif ((percentage / 10) / j > 1) printf(\"#\");\n",
        "\t\t\telse printf(\" \");\n",
        "\t\t}\n",
        "\t\tprintf(\" [%.2f%%]\", percentage);\n",
        "\t\tfflush(stdout);\n",
        "\t}\n",
        "\tCUDA_RUN(cudaEventRecord(stopEvent, 0));\n",
        "\tCUDA_RUN(cudaEventSynchronize(stopEvent));\n",
        "\tCUDA_RUN(cudaEventElapsedTime(&time_passed, startEvent, stopEvent));\n",
        "\tCUDA_RUN(cudaMemcpy(shortestPath, dev_shortestPath, num_blocks * size * size8, cudaMemcpyDeviceToHost));\n",
        "\tCUDA_RUN(cudaMemcpy(cost, dev_cost, num_blocks * size32, cudaMemcpyDeviceToHost));\n",
        "\n",
        "\tprintf(\"\\nTime passed:  %3.1f ms \\n\", time_passed);\n",
        "\tprint_Graph(graphWeights, size);\n",
        "\n",
        "\t{\n",
        "\t\tint32_t min = cost[0];\n",
        "\t\tint8_t index = 0;\n",
        "\t\tfor (int i = 1; i < num_blocks; i++) {\n",
        "\t\t\tif (cost[i] < min) {\n",
        "\t\t\t\tmin = cost[i];\n",
        "\t\t\t\tindex = i;\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t\tprintf(\"Shortest path found on block #%d:\\n\", index + 1);\n",
        "\t\tprint_ShortestPath(&shortestPath[index * size], min, size);\n",
        "\t}\n",
        "\n",
        "Error:\n",
        "\tfree(city_ids);\n",
        "\tfree(shortestPath);\n",
        "\tfree(graphWeights);\n",
        "\tfree(cost);\n",
        "\tfree(choices);\n",
        "\n",
        "\tcudaFree(dev_city_ids);\n",
        "\tcudaFree(dev_shortestPath);\n",
        "\tcudaFree(dev_graphWeights);\n",
        "\tcudaFree(dev_cost);\n",
        "\tcudaFree(dev_size);\n",
        "\tcudaFree(dev_selected_K);\n",
        "\tcudaFree(dev_choices);\n",
        "\tcudaFree(dev_threads_per_kernel);\n",
        "\n",
        "\tcudaEventDestroy(startEvent);\n",
        "\tcudaEventDestroy(stopEvent);\n",
        "\n",
        "\tgetchar();\n",
        "\n",
        "\treturn 0;\n",
        "}\n",
        "\n",
        "__global__\n",
        "void find_permutations_for_threads(int8_t * city_ids, int8_t * k, int8_t * choices, int32_t * size, unsigned long long * threads_per_kernel) {\n",
        "\tint32_t length = *size;\n",
        "\tint8_t index = 1;\n",
        "\tunsigned long long count = 0;\n",
        "\tfor (count = 0; count < *threads_per_kernel; count++) {\n",
        "\t\tfor (int i = 0; i < length; i++) {\n",
        "\t\t\tchoices[i + count * length] = city_ids[i];\n",
        "\t\t}\n",
        "\t\treverse(city_ids + *k + index, city_ids + length);\n",
        "\t\tnext_permutation(city_ids + index, city_ids + length);\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__\n",
        "void combinations_kernel(int8_t * choices, int8_t * k, int8_t * shortestPath, int8_t * graphWeights, int32_t * cost, int32_t * size) {\n",
        "\tuint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint32_t length = *size;\n",
        "\tint8_t index = 1;\n",
        "\n",
        "\t/* local variables */\n",
        "\tint8_t * _path, *_shortestPath;\n",
        "\tint32_t _tcost;\n",
        "\n",
        "\tSAFE(_path = (int8_t *)malloc(length * sizeof(int8_t)));\n",
        "\tSAFE(_shortestPath = (int8_t *)malloc(length * sizeof(int8_t)));\n",
        "\t_tcost = length * 100;\n",
        "\n",
        "\tmemcpy(_path, choices + tid * length, length * sizeof(int8_t));\n",
        "\tmemcpy(_shortestPath, shortestPath, length * sizeof(int8_t));\n",
        "\n",
        "\tif (threadIdx.x == 0) {\n",
        "\t\tif (cost[blockIdx.x] == 0) cost[blockIdx.x] = length * 100;\n",
        "\t\tshared_cost = length * 100;\n",
        "\t}\n",
        "\n",
        "\t__syncthreads();\n",
        "\n",
        "\tdo {\n",
        "\t\tcoppy_array(_path, _shortestPath, &_tcost, graphWeights, length, tid);\n",
        "\t} while (next_permutation(_path + *k + index, _path + length));\n",
        "\n",
        "\tif (_tcost == shared_cost) {\n",
        "\t\tatomicMin(&cost[blockIdx.x], _tcost);\n",
        "\t\tif (cost[blockIdx.x] == _tcost) {\n",
        "\t\t\tmemcpy(shortestPath + blockIdx.x * length, _shortestPath, length * sizeof(int8_t));\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfree(_path);\n",
        "\tfree(_shortestPath);\n",
        "}\n",
        "\n",
        "__host__\n",
        "void initialize(int8_t * city_ids, int8_t * graphWeights, int32_t size) {\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tcity_ids[i] = i;\n",
        "\t\tfor (int j = 0; j < size; j++) {\n",
        "\t\t\tif (i == j)\n",
        "\t\t\t\tgraphWeights[i * size + j] = 0;\n",
        "\t\t\telse\n",
        "\t\t\t\tgraphWeights[i * size + j] = 99;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = 0; i < size; i++) {\n",
        "\t\tfor (int j = 0; j < size;) {\n",
        "\t\t\tint next = 1; // (rand() % 2) + 1;\n",
        "\t\t\tint road = rand() % 100 + 1;\n",
        "\t\t\tif (i == j) {\n",
        "\t\t\t\tj += next;\n",
        "\t\t\t\tcontinue;\n",
        "\t\t\t}\n",
        "\t\t\tgraphWeights[i * size + j] = road;\n",
        "\t\t\tj += next;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tfor (int i = size - 1; i >= 0; i--) {\n",
        "\t\tgraphWeights[((i + 1) % size) * size + i] = 1;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__host__\n",
        "void print_Graph(int8_t * graphWeights, int32_t size) {\n",
        "\tint i, j;\n",
        "\tfor (i = 0; i < size; i++) {\n",
        "\t\tfor (j = 0; j < size; j++) {\n",
        "\t\t\tprintf(\"%d\\t\", graphWeights[i * size + j]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "}\n",
        "\n",
        "__host__\n",
        "void print_ShortestPath(int8_t * shortestPath, int32_t cost, int32_t size) {\n",
        "\tint i;\n",
        "\tif (cost == (size * 100)) printf(\"no possible path found.\\n\");\n",
        "\telse {\n",
        "\t\tfor (i = 0; i < size; i++) {\n",
        "\t\t\tprintf(\"%d\\t\", shortestPath[i]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\nCost: %d\\n\", cost);\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__\n",
        "void swap(int8_t *x, int8_t *y) { int8_t tmp = *x; *x = *y;\t*y = tmp; }\n",
        "\n",
        "__device__\n",
        "void reverse(int8_t *first, int8_t *last) { while ((first != last) && (first != --last)) swap(first++, last); }\n",
        "\n",
        "__device__\n",
        "void coppy_array(int8_t * path, int8_t * shortestPath, int32_t * tcost, int8_t * weights, int8_t length, int tid) {\n",
        "\tint32_t sum = 0;\n",
        "\tfor (int32_t i = 0; i < length; i++) {\n",
        "\t\tint8_t val = weights[path[i] * length + path[(i + 1) % length]];\n",
        "\t\tif (val == -1) return;\n",
        "\t\tsum += val;\n",
        "\t}\n",
        "\tif (sum == 0) return;\n",
        "\tatomicMin(&shared_cost, sum);\n",
        "\tif (shared_cost == sum) {\n",
        "\t\t*tcost = sum;\n",
        "\t\tmemcpy(shortestPath, path, length * sizeof(int32_t));\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__\n",
        "bool next_permutation(int8_t * first, int8_t * last) {\n",
        "\tif (first == last) return false;\n",
        "\tint8_t * i = first;\n",
        "\t++i;\n",
        "\tif (i == last) return false;\n",
        "\ti = last;\n",
        "\t--i;\n",
        "\n",
        "\tfor (;;) {\n",
        "\t\tint8_t * ii = i--;\n",
        "\t\tif (*i < *ii) {\n",
        "\t\t\tint8_t * j = last;\n",
        "\t\t\twhile (!(*i < *--j));\n",
        "\t\t\tswap(i, j);\n",
        "\t\t\treverse(ii, last);\n",
        "\t\t\treturn true;\n",
        "\t\t}\n",
        "\t\tif (i == first) {\n",
        "\t\t\treverse(first, last);\n",
        "\t\t\treturn false;\n",
        "\t\t}\n",
        "\t}\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nWF_PExHLeL",
        "outputId": "177e1f4a-8285-4aaf-b93c-c5f09bfee886"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also check the specs of the GPU assigned to you using this code:"
      ],
      "metadata": {
        "id": "seHKhfo7mbG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU Specs\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJfhJkcgmOJp",
        "outputId": "aabd26bd-afff-47d1-da94-f10dd234a029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 24 21:44:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see anything but \"GPU ENABLED - ..etc\", then repeat the above steps again."
      ],
      "metadata": {
        "id": "3Gb610rHmAXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# SANDBOX\n",
        "Now you can try your own CUDA code in the box below. \n",
        "Note that %%cu is used to switch the mode to CUDA"
      ],
      "metadata": {
        "id": "oS0kJx-HdIkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "SONrkrP1IfZu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd8bMrV9M6U-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fab0ae-bf04-4807-b0b1-a9c0d712620e"
      },
      "source": [
        "%%cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "//you may use this macro for error checking\n",
        "#define CHK(call) {cudaError_t err = call; if (err != cudaSuccess) { printf(\"Error%d: %s:%d\\n\",err,__FILE__,__LINE__); printf(cudaGetErrorString(err)); cudaDeviceReset(); exit(1);}}\n",
        "__global__ void kernel(){\n",
        "}\n",
        "\n",
        "int main(){\n",
        "   \n",
        "    //TODO: add your code here and modify the kernel launch below\n",
        "    kernel<<<1,1>>>();\n",
        "    CHK(cudaGetLastError());\n",
        "    CHK(cudaDeviceSynchronize());\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#EXAMPLE: Vector Addition\n",
        "Here is a sample program for demonstration. "
      ],
      "metadata": {
        "id": "5eKeeAp4dmUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Serial Code"
      ],
      "metadata": {
        "id": "54hsYo44eQ1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#define N 1024\n",
        "\n",
        "void vectorAdd(int* a, int* b, int* c, int n) {\n",
        "   int i;\n",
        "   for (i = 0; i < n; i++)\n",
        "\tc[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "   int *a = (int*) malloc(N * sizeof(int));\t//create three arrays\n",
        "   int *b = (int*) malloc(N * sizeof(int));\n",
        "   int *c = (int*) malloc(N * sizeof(int));\n",
        "   \n",
        "   for(int i = 0; i < N; i++) \t//intialize a,b \n",
        "      a[i] = b[i] = i;\n",
        "\n",
        "   vectorAdd(a, b, c, N);\t\t// vector addition\n",
        "\n",
        "   for(int i = 0; i < 10; i++)\t// print first 10 elements\n",
        "\t    printf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\n",
        "   free(a);free(b);free(c); \t// free memory taken by a, b, c\n",
        "   return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "2Vb6xf5Yds0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Parallel Code *WITH Unified Memory*"
      ],
      "metadata": {
        "id": "2ohRvavNgRgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include <stdio.h>\n",
        "#define N 1024\n",
        "#define CHK(call) {cudaError_t err = call; if (err != cudaSuccess) { printf(\"Error%d: %s:%d\\n\",err,__FILE__,__LINE__); printf(cudaGetErrorString(err)); cudaDeviceReset(); exit(1);}}\n",
        "\n",
        "__global__ void vectorAdd() {\n",
        "  \tint i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i<N){\n",
        "      __shared__ int array[256];\n",
        "      int *p = &array[i];\n",
        "      *p = i;\n",
        "      printf(\"%d \", *p);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "   vectorAdd<<<3,10>>>();      // run N threads on 1 block\n",
        "   CHK(cudaGetLastError());\t\t\t//1\n",
        "   CHK(cudaDeviceSynchronize());\t//2\n",
        "\n",
        "   return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLPZZjWXoeXK",
        "outputId": "57909338-36e4-4548-c3c0-5b9257d38806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 0 1 2 3 4 5 6 7 8 9 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include <stdio.h>\n",
        "#define N 1024\n",
        "\n",
        "__global__ void vectorAdd(int* a, int* b, int* c, int n) {\n",
        "  \tint i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i<n)\n",
        "      c[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int *a, *b, *c;\n",
        "\n",
        "    // allocate space on unified memory\n",
        "    cudaMallocManaged(&a, N * sizeof(int));      \n",
        "    cudaMallocManaged(&b, N * sizeof(int));\n",
        "    cudaMallocManaged(&c, N * sizeof(int));\n",
        "\n",
        "    //initialize a, b (for testing)\n",
        "    for(int i = 0; i < N; i++) \t                  \n",
        "        a[i] = b[i] = i;\n",
        "\n",
        "    // launch kernel\n",
        "    vectorAdd<<<1,N>>>(a, b, c, N);      // run N threads on 1 block\n",
        "    cudaDeviceSynchronize();             // Wait for GPU to finish before accessing on host\n",
        "\n",
        "    // print first 10 elements(for testing)\n",
        "    for(int i=0; i<10; i++)\t                      \n",
        "\t    printf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "    \n",
        "    //free device memory\n",
        "    cudaFree(a); cudaFree(b); cudaFree(c);        \n",
        "\n",
        "   return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "yDyNPuzCgSDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cd205b-5aaa-40a8-c02b-8bb79aafb6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c[0] = 0\n",
            "c[1] = 2\n",
            "c[2] = 4\n",
            "c[3] = 6\n",
            "c[4] = 8\n",
            "c[5] = 10\n",
            "c[6] = 12\n",
            "c[7] = 14\n",
            "c[8] = 16\n",
            "c[9] = 18\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Parallel Code *WITHOUT Unified Memory*"
      ],
      "metadata": {
        "id": "bya10w3Ne2dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include <stdio.h>\n",
        "#define N 1024\n",
        "\n",
        "__global__ void vectorAdd(int* a, int* b, int* c, int n) {\n",
        "  \tint i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i<n)\n",
        "      c[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        " int main() {\n",
        "    int *a, *b, *c, *d_A, *d_B, *d_C;\t     \n",
        "    \n",
        "    // allocate space on host\n",
        "    a = (int*) malloc(N * sizeof(int)); \t\n",
        "    b = (int*) malloc(N * sizeof(int));\n",
        "    c = (int*) malloc(N * sizeof(int));\n",
        "    \n",
        "    // allocate space on device\n",
        "    cudaMalloc(&d_A, N * sizeof(int));      \n",
        "    cudaMalloc(&d_B, N * sizeof(int));\n",
        "    cudaMalloc(&d_C, N * sizeof(int));\n",
        "\n",
        "    //initialize a, b (for testing)\n",
        "    for(int i = 0; i < N; i++) \t                  \n",
        "        a[i] = b[i] = i;\n",
        "\n",
        "    //copy data from host to device\n",
        "    cudaMemcpy(d_A, a, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, b, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    //launch the kernel (with pointers to device memory)\n",
        "    vectorAdd<<<1,N>>>(d_A, d_B, d_C, N);         //run N threads on 1 block\n",
        "\n",
        "    //copy results from device to host \n",
        "    cudaMemcpy(c, d_C, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print first 10 elements(for testing)\n",
        "    for(int i=0; i<10; i++)\t                      \n",
        "\t    printf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\n",
        "    //free memory\n",
        "    free(a);free(b);free(c);                      // host memory\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);  // device memory\n",
        "\n",
        "   return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "biLMHy0te5G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Device Properties\n",
        "You can use this code to get the properties of the GPU currenlty allocated to you (this is the same code from the assignment)."
      ],
      "metadata": {
        "id": "IvnDZbSQynVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "int main() {\n",
        "\tcudaDeviceProp prop;\n",
        "\tint count;\n",
        "\tcudaGetDeviceCount(&count);\n",
        "\tfor (int i = 0; i < count; i++) {\n",
        "\t\tcudaGetDeviceProperties(&prop, i);\n",
        "\t\tprintf(\"----- General Information for device %d ---\\n\", i);\n",
        "\t\tprintf(\"Name:                     %s\\n\", prop.name);\n",
        "\t\tprintf(\"Compute capability:       %d.%d\\n\", prop.major, prop.minor);\n",
        "\t\tprintf(\"Clock rate:               %d\\n\", prop.clockRate);\n",
        "\t\tprintf(\"Device copy overlap:      \");\n",
        "\t\tprintf(prop.deviceOverlap ? \"Enabled\\n\" : \"Disabled\\n\");\n",
        "\t\tprintf(\"Kernel execution timeout: \");\n",
        "\t\tprintf(prop.kernelExecTimeoutEnabled ? \"Enabled\\n\" : \"Disabled\\n\");\n",
        "\t\tprintf(\"----- Memory Information for device %d ---\\n\", i);\n",
        "\t\tprintf(\"Total global mem:         %lu\\n\", prop.totalGlobalMem);\n",
        "\t\tprintf(\"Total constant Mem:       %ld\\n\", prop.totalConstMem);\n",
        "\t\tprintf(\"Max mem pitch:            %ld\\n\", prop.memPitch);\n",
        "\t\tprintf(\"Texture Alignment:        %ld\\n\", prop.textureAlignment);\n",
        "\t\tprintf(\"----- MP Information for device %d ---\\n\", i);\n",
        "\t\tprintf(\"Multiprocessor count:     %d\\n\", prop.multiProcessorCount);\n",
        "\t\tprintf(\"Shared mem per mp:        %ld\\n\", prop.sharedMemPerBlock);\n",
        "\t\tprintf(\"Registers per mp:         %d\\n\", prop.regsPerBlock);\n",
        "\t\tprintf(\"Threads in warp:          %d\\n\", prop.warpSize);\n",
        "\t\tprintf(\"Max threads per block:    %d\\n\", prop.maxThreadsPerBlock);\n",
        "\t\tprintf(\"Max thread dimensions:    (%d, %d, %d)\\n\",\n",
        "\t\t\tprop.maxThreadsDim[0], prop.maxThreadsDim[1], \t\t\t\t\tprop.maxThreadsDim[2]);\n",
        "\t\tprintf(\"Max grid dimensions:      (%d, %d, %d)\\n\",\n",
        "\t\t\tprop.maxGridSize[0], prop.maxGridSize[1],\n",
        "\t\t\tprop.maxGridSize[2]);\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "KmZDJ1eyympL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}